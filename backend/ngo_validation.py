# # -*- coding: utf-8 -*-
# """ngo_verification.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1q_Pm7cKgA_mOjVN6Uurnwwq2uOMfs_bz
# """

# # Step 1: Install Linux dependencies required for Chromium (if not already installed)
# !apt-get update && apt-get install -y \
#     libnss3 libatk1.0-0 libatk-bridge2.0-0 libcups2 libdrm2 libxkbcommon0 \
#     libxcomposite1 libxrandr2 libgbm1 libasound2

# # Step 2: Install Playwright and nest_asyncio, and install browser binaries
# !pip install playwright nest_asyncio
# !playwright install

# # Step 3: Import modules and apply nest_asyncio to allow asynchronous code in Colab
# import nest_asyncio
# nest_asyncio.apply()

# import asyncio
# from playwright.async_api import async_playwright
# import pandas as pd

# async def scrape_ngos():
#     async with async_playwright() as p:
#         # Launch headless Chromium
#         browser = await p.chromium.launch(headless=True)
#         page = await browser.new_page()
#         url = "https://ngosindia.org/ngo-profile/ngo-verification/maharashtra/"
#         print("Navigating to:", url)
#         await page.goto(url, timeout=60000)

#         # Wait for the page to reach network idle
#         await page.wait_for_load_state("networkidle", timeout=60000)

#         # Save page content for debugging purposes
#         html_content = await page.content()
#         with open("page_debug.html", "w", encoding="utf-8") as f:
#             f.write(html_content)
#         print("Page content saved to 'page_debug.html'. Please download and inspect it to locate NGO data.")

#         # Attempt to find a table element (update selector if needed)
#         tables = await page.query_selector_all("table")
#         ngos = []
#         if tables:
#             print("Found", len(tables), "table(s) on the page.")
#             # Iterate over tables and try to extract NGO data
#             for table in tables:
#                 rows = await table.query_selector_all("tr")
#                 if len(rows) > 1:
#                     print("Using a table with", len(rows), "rows.")
#                     # Skip header row if present
#                     for row in rows[1:]:
#                         cols = await row.query_selector_all("td")
#                         if len(cols) >= 5:
#                             unique_id = (await cols[0].inner_text()).strip()
#                             ngo_name  = (await cols[1].inner_text()).strip()
#                             state     = (await cols[2].inner_text()).strip()
#                             address   = (await cols[3].inner_text()).strip()
#                             sectors   = (await cols[4].inner_text()).strip()
#                             ngos.append({
#                                 "Unique ID": unique_id,
#                                 "NGO Name": ngo_name,
#                                 "State": state,
#                                 "Address": address,
#                                 "Sectors": sectors,
#                             })
#                     # Exit after processing one table if data is found
#                     if ngos:
#                         break
#         else:
#             print("No table found on the page.")

#         if not ngos:
#             print("No NGO data extracted. The page structure may have changed. Please inspect 'page_debug.html'.")

#         await browser.close()
#         return ngos

# # Run the async function using the existing event loop in Colab
# ngos = asyncio.get_event_loop().run_until_complete(scrape_ngos())

# print("Scraped", len(ngos), "NGOs.")
# if ngos:
#     df = pd.DataFrame(ngos)
#     df.to_csv("verified_ngos.csv", index=False)
#     print("Data saved to 'verified_ngos.csv'.")
# else:
#     print("No NGO data found. Please inspect the saved HTML (page_debug.html) to update the selectors.")

# from bs4 import BeautifulSoup
# import pandas as pd

# # Open and read the saved HTML file
# with open("page_debug.html", "r", encoding="utf-8") as f:
#     html_content = f.read()

# # Parse the HTML content
# soup = BeautifulSoup(html_content, "html.parser")

# # Locate the main content area containing the NGO listings.
# # In this case, the listings appear inside a <div> with class "entry clearfix"
# entry_div = soup.find("div", class_="entry clearfix")
# if entry_div:
#     ngo_list = []
#     # Find all unordered lists (<ul>) within this content
#     for ul in entry_div.find_all("ul"):
#         # Iterate over each list item (<li>) in the list
#         for li in ul.find_all("li"):
#             a_tag = li.find("a")
#             if a_tag and a_tag.text:
#                 ngo_name = a_tag.get_text(strip=True)
#                 ngo_link = a_tag.get("href")
#                 ngo_list.append({"NGO Name": ngo_name, "Link": ngo_link})

#     if ngo_list:
#         # Create a DataFrame and display some of the data
#         df = pd.DataFrame(ngo_list)
#         print("Extracted NGO Data:")
#         print(df.head())

#         # Save the data to a CSV file
#         df.to_csv("maharashtra_ngos.csv", index=False)
#         print("Data saved to 'maharashtra_ngos.csv'.")
#     else:
#         print("No NGO list items found in the content.")
# else:
#     print("No entry content found in the HTML.")
import asyncio
from playwright.async_api import async_playwright
import pandas as pd

async def scrape_ngos():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        url = "https://ngosindia.org/ngo-profile/ngo-verification/maharashtra/"
        await page.goto(url, timeout=60000)

        await page.wait_for_load_state("networkidle", timeout=60000)
        
        tables = await page.query_selector_all("table")
        ngos = []
        if tables:
            for table in tables:
                rows = await table.query_selector_all("tr")
                for row in rows[1:]:  # Skip header
                    cols = await row.query_selector_all("td")
                    if len(cols) >= 5:
                        unique_id = (await cols[0].inner_text()).strip()
                        ngo_name  = (await cols[1].inner_text()).strip()
                        state     = (await cols[2].inner_text()).strip()
                        address   = (await cols[3].inner_text()).strip()
                        sectors   = (await cols[4].inner_text()).strip()
                        ngos.append({
                            "Unique ID": unique_id,
                            "NGO Name": ngo_name,
                            "State": state,
                            "Address": address,
                            "Sectors": sectors,
                        })
                if ngos:
                    break
        await browser.close()
        return ngos  # Returning the data instead of saving CSV
